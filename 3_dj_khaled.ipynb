{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, platform, time, urllib.request, openpyxl, operator\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from openpyxl import Workbook\n",
    "import sys, requests, re, json\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape general information\n",
    "def scrape_general_info(url):\n",
    "    # instagram URL \n",
    "    URL = url\n",
    "\n",
    "    # creating a dictionary \n",
    "    data = {} \n",
    "\n",
    "    # getting the request from url \n",
    "    r = requests.get(URL)\n",
    "\n",
    "    # converting the text \n",
    "    s = BeautifulSoup(r.text, \"html.parser\") \n",
    "\n",
    "    # finding meta info \n",
    "    meta = s.find(\"meta\", property =\"og:description\")\n",
    "\n",
    "    #searching followers, followeing and number of posts info\n",
    "    meta_2 = meta.attrs['content']\n",
    "    meta_3 = meta_2.split(\"-\")[0].split(\" \")\n",
    "\n",
    "    # assigning the values \n",
    "    data['Followers'] = meta_3[0] \n",
    "    data['Following'] = meta_3[2] \n",
    "    data['Posts'] = meta_3[4]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Empty List\n",
    "link = []\n",
    "names = []\n",
    "\n",
    "def get_influencer_link(username):\n",
    "    #to influencer url\n",
    "    url = f'https://www.instagram.com/{username}/'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    i = 0\n",
    "    while i < 8:   \n",
    "        try:\n",
    "            #get the links\n",
    "            pages = driver.find_elements_by_tag_name('a')\n",
    "            for data in pages:\n",
    "                data_2 = data.get_attribute(\"href\")\n",
    "                if '/p/' in data_2:\n",
    "                    link.append(data.get_attribute(\"href\"))\n",
    "                    names.append(username)\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(1)\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "    driver.quit()\n",
    "\n",
    "    return link, names\n",
    "\n",
    "\n",
    "likes = []\n",
    "comment_counts = []\n",
    "dates = []\n",
    "captions = []\n",
    "type_posts = []\n",
    "links = []\n",
    "i = 0\n",
    "n = 0\n",
    "\n",
    "def get_information(link):    \n",
    "    try:\n",
    "        global i, n\n",
    "        \n",
    "        #accessing and parsing the website url\n",
    "        url = link\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        \n",
    "        #find element that contain information\n",
    "        body = soup.find('body')\n",
    "        script = body.find('script')\n",
    "        raw = script.text.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "        json_data=json.loads(raw)\n",
    "        posts =json_data['entry_data']['PostPage'][0]['graphql']\n",
    "        posts= json.dumps(posts)\n",
    "        posts = json.loads(posts)\n",
    "        \n",
    "        #acquiring information\n",
    "        like = posts['shortcode_media']['edge_media_preview_like']['count']\n",
    "        comment_count = posts['shortcode_media']['edge_media_to_parent_comment']['count']\n",
    "        date = posts['shortcode_media']['taken_at_timestamp']\n",
    "        caption = posts['shortcode_media']['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "        type_post = posts['shortcode_media']['__typename']\n",
    "        likes.append(like)\n",
    "        comment_counts.append(comment_count)\n",
    "        dates.append(date)\n",
    "        captions.append(caption)\n",
    "        type_posts.append(type_post)\n",
    "        links.append(link)\n",
    "        i += 1\n",
    "    except:\n",
    "        i += 1\n",
    "        n += 1\n",
    "        print(f'number of link error {n} at iteration {i}')\n",
    "        pass\n",
    "    return likes, comment_counts, dates, captions, type_posts, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for name in username.table1['username_2']:\n",
    "    # user name \n",
    "    url = f\"https://www.instagram.com/{name}/\"\n",
    "\n",
    "    # calling scrape function\n",
    "    try:\n",
    "        data = scrape_general_info(url)\n",
    "        username.table1.loc[i, 'followers'] = data['Followers']\n",
    "        username.table1.loc[i, 'following'] = data['Following']\n",
    "        username.table1.loc[i, 'post'] = data['Posts']\n",
    "        i += 1\n",
    "    except:\n",
    "        i += 1\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ironhack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f61e768a7a786be35d65123db8f98311986f4db302470b02f0c043ce57ff1dee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
